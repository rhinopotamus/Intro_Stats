---
title: "22. Regression"
author: "Put your name here"
date: "Put the date here"
output:
    html_notebook:
        toc: yes
        toc_float: yes
---

<!-- Please don't mess with the next few lines! -->
<style>h5{font-size:2em;color:#0000FF}h6{font-size:1.5em;color:#0000FF}div.answer{margin-left:5%;border:1px solid #0000FF;border-left-width:10px;padding:25px} div.summary{background-color:rgba(30,144,255,0.1);border:3px double #0000FF;padding:25px}</style>`r options(scipen=999)`<p style="color:#ffffff">`r intToUtf8(c(49,46,52))`</p>
<!-- Please don't mess with the previous few lines! -->

<div class = "summary">
### Functions introduced in this module:
`geom_smooth`, `lm`, `augment`, `glance`
</div>


## Introduction

In this module we will learn how to run a regression analysis. Regression provides a model for the linear relationship between two numerical variables.


## Load packages

We load the standard `mosaic` package as well as the `openintro` package for the `bdims` data, the `reshape2` package for the `tips` data, and the `MASS` package for the `Rubber` data. The `broom` package gives us tidy output.

```{r, warning = FALSE, message = FALSE}
library(openintro)
library(reshape2)
library(MASS)
library(broom)
library(mosaic)
```


## Research question

Can you predict someone's weight from their wrist diameter?


## Regression

When we have a linear relationship between two numerical variables, it's helpful to model this relationship with an actual straight line. Such a line is called a regression line, or a best-fit line, or sometimes a least-squares line.

The mathematics involved in figuring out what this line should be is more complicated than we cover in these modules. But R will do all the complicated calculations for us.

Let's look at a scatterplot of data from the `bdims` data set. These measurements come from 507 physically active individuals (a mix of males and females). The weight `wgt` is measured in kilograms and the wrist diameter `wri_di` is measured in centimeters as the sum of both wrists.

```{r}
ggplot(bdims, aes(y = wgt, x = wri_di)) +
    geom_point()
```

If certain conditions are met, we can graph a regression line; just add a `geom_smooth` layer to the scatterplot:

```{r}
ggplot(bdims, aes(y = wgt, x = wri_di)) +
    geom_point() +
    geom_smooth(method = "lm")
```

The `method = "lm"` argument is telling `ggplot` to use a "linear model".

Of all possible lines, the blue line comes the closest to each point in the scatterplot. If we wiggled the line a little bit, it might get closer to a few points, but the net effect would be to make it further from other points. This is the mathematically optimal line of best fit. The gray region around the line is called a confidence band; assuming our sample is a random sample from the population, the "true" population regression line is likely to lie inside this gray area.

What is the equation of this line? In your algebra class you learn that a line takes the form $y = mx + b$ where $m$ is the slope and $b$ is the y-intercept. Statisticians write the equation in a slightly different form:

$$\hat{y} = b_{0} + b_{1} x.$$

The intercept is $b_{0}$ and the slope is $b_{1}$. We use $\hat{y}$ instead of $y$ because when we plug in values of $x$, we do not get back the exact values of $y$ from the data. The line, after all, does not actually pass through most (if any) actual data points. Instead, this equation gives us "predicted" values of $y$ that lie on the regression line. These predicted $y$ values are called $\hat{y}$.

To run a regression analysis, we use the `lm` command in R. (`lm` stands for "linear model".) As usual, we then use the `tidy` command to make the output a little cleaner. There is one tiny change we'll make to the `tidy` command: unlike previous hypothesis tests we've seen, the `tidy` command for `lm` does not automatically include a confidence interval. So we'll need to add an argument `conf.int = TRUE`.

```{r}
wgt_wrist_lm <- lm(wgt ~ wri_di, data = bdims)
wgt_wrist_tidy <- tidy(wgt_wrist_lm, conf.int = TRUE)
wgt_wrist_tidy
```


## Interpreting the coefficients

Look at the output of the `tidy` command above.

The `estimate` column of the output gives us the intercept and slope for the model regression line.

```{r}
wgt_wrist_tidy$estimate[1]
```

```{r}
wgt_wrist_tidy$estimate[2]
```

The intercept is `r wgt_wrist_tidy$estimate[1]` and the slope is `r wgt_wrist_tidy$estimate[2]`.

Therefore, the equation of the regression line could be written

$$\hat{y} = -44.77 + 10.81 x.$$

When we report the equation of the regression line, we typically use words instead of $\hat{y}$ and $x$ to make the equation more interpretable in the context of the problem. For example, for this data, we would write the equation as

$$\widehat{weight} = -44.77 + 10.81 wrist.$$

The slope $b_{1}$ is always interpretable. This model predicts that one unit of increase in the x-direction corresponds to a change of 10.81 units in the y-direction. Let's phrase it this way, using inline code:

> The model predicts that an increase of one cm in wrist diamater corresponds to a weight increase of `r wgt_wrist_tidy$estimate[2]` kg.

The intercept $b_{0}$ is a different story. There is always a literal interpretation:

> The model predicts that someone with a wrist diameter of zero will weigh `r wgt_wrist_tidy$estimate[1]` kg.

It is true that the model makes that prediction, so the preceding sentence is technically correct. However, that prediction is nonsensical. Aside from the fact that it is impossible for a wrist to have zero diameter and impossible for a weight to be negative, this is *extrapolation*---in other words, a prediction outside the range of the data.

The rest of the `tidy` output involves inference on the two parameters (intercept and slope). We'll get back to that later.


## Checking conditions

We need to be careful here. We have not checked any conditions; therefore, it is inappropriate to fit a regression line at this point. Once the line is seen, it cannot easily be "unseen", and it's crucial that you don't trick your reader into believing there is a linear relationship before checking the conditions that justify that belief. 

The regression line we saw above makes no sense unless we know that regression is appropriate. The conditions for running a regression analysis include all the conditions you checked for a correlation analysis: "Random" and "10%" to ensure a good sample, "Linear association" to make sure our association follows a linear pattern, and an "Outlier" check because outliers can be bad. Let's check what he have so far:

* Random
    -  We are not told how the sample was collected, so we can't say if it's a representative sample of all physically active individuals. We'll proceed with caution.

* 10%
    - Surely `r NROW(bdims)` people are less than 10% of all physically active people.

* Linear association
    - The scatterplot showed a clear linear relationship between wrist diameter and weight.

* Outliers
    - There are no apparent outliers in the scatterplot.

However, there is an additional condition to check to ensure that our regression model is appropriate. We need to check for...

* Patterns in the residuals

We discuss this below.


## Residuals

Residuals are the vertical distances from each data point to the regression line. More formally, we're saying that the residual $e$ is given by the following formula:

$$e = y - \hat{y}.$$

We know that some of the points are going to lie above the line (positive residuals) and some of the points will lie below the line (negative residuals). What we need is for there not to be any pattern among these residuals.

To calculate the residuals, we introduce a new function from the `broom` package. Whereas `tidy` serves up information about the parameters of interest (in this case, the intercept and the slope of the regression line), `augment` gives us extra information for each data point.

```{r}
wgt_wrist_aug <- augment(wgt_wrist_lm)
wgt_wrist_aug
```

The first two columns are the actual data values we started with. But now we've "augmented" the original data with some new stuff too. The third column---here called `.fitted`---is $\hat{y}$, or the point on the line that corresponds to the given $x$ value. Let's check and make sure this is working as advertised.

The regression equation from above is

$$\widehat{weight} = -44.77 + 10.81 wrist.$$

The first subject listed in row 1 has wrist diameter 10.4 cm. Plug that value into the equation above:

$$\widehat{weight} = -44.77 + 10.81(10.4) = 67.6.$$

The model predicts that a person with a wrist diameter of 10.4 will weigh 67.6 kg. The first number in the `.fitted` column is 67.60664, so that's correct.

Now skip over to the fifth column of the `augment` output, the one that says `.resid`. If this is the residual $e$, then it should be $y - \hat{y}$. Since $y$ is the actual value of `wgt` and $\hat{y}$ is the value predicted by the model, we should get for the first row of output

$$e = y - \hat{y} = 65.6 - 67.60664 = -2.00664.$$

Yup, it works!

Skip to the last column of the output `.std.resid`. (You may have to click on the little black arrow to scroll all the way to the right.) These are similar to z-scores for the residuals, so they're easy to interpret.^[The real story is quite a bit more complicated than that, but there is little harm in thinking of these as if they were z-scores.] One can examine either the raw residuals (stored in `.resid`) or the standardized residuals (`.std.resid`), but we'll do the latter.

To check for patterns in the residuals, we'll use several different plots. First, we want our residuals to be normally distributed. We check this with a histogram and a QQ plot, as usual.

```{r}
ggplot(wgt_wrist_aug, aes(x = .std.resid)) +
    geom_histogram(binwidth = 0.5, boundary = 0)
```

```{r}
ggplot(wgt_wrist_aug, aes(sample = .std.resid)) +
    geom_qq() +
    geom_qq_line()
```

We see that the shape is mostly normal. There's a little bit of skew in the histogram, but nothing alarming.

We should also create a *residual plot*, which looks at the residuals above each value along the x-axis. (In the command below, we also add a horizontal reference line so that it is clear which points have positive or negative residuals.)

```{r}
ggplot(wgt_wrist_aug, aes(y = .std.resid, x = wri_di)) +
    geom_point() +
    geom_hline(yintercept = 0)
```

This looks good. There are no systematic patterns in the residuals. A residual plot should look like the most boring plot you've ever seen.

Residual patterns that are problematic often involve curved data (where the dots follow a curve around the horizontal reference line instead of spreading evenly around it) and *heteroscedasticity*, which is a fanning out pattern.

As an example of the latter, let's look at the residual plot for the tipping example from the correlation module.

```{r}
tips <- reshape2::tips
tip_bill <- lm(tip ~ total_bill, data = tips)
tip_bill_aug <- augment(tip_bill)
ggplot(tip_bill_aug, aes(y = .std.resid, x = total_bill)) +
    geom_point() +
    geom_hline(yintercept = 0)
```

The residuals are quite small toward the left end of the residual plot, and then spread out and get larger toward the right end. This is a violation of the "patterns in the residuals" condition. It would be problematic to pursue a regression analysis of the tip data even though correlation was okay to report.


## $R^2$

The correlation coefficient R is of limited utility. The number doesn't have any kind of intrinsic meaning; it can only be judged by how close it is to 0 or 1 (or -1) in conjunction with a scatterplot to give you a sense of the strength of the correlation. In particular, some people try to interpret R as some kind of percentage, but it's not.

On the other hand, $R^2$ can be interpreted as a percentage. It represents the percent of variation in the y variable that can be explained by variation in the x variable.

Here we introduce the last of the `broom` functions: `glance`. Whereas `tidy` reports summary statistics related to parameters of the model, and `augment` reports values associated to each data point separately, the `glance` function gathers up summaries for the entire model.

```{r}
wgt_wrist_glance <- glance(wgt_wrist_lm)
wgt_wrist_glance
```

A more advanced statistics course might discuss the other model summaries present in the `glance` output. The $R^{2}$ value is stored in `wgt_wrist_glance$r.squared`. Its value is 0.585. We will word it this way:

> `r 100*wgt_wrist_glance$r.squared`% of the variability in weight can be explained by variability in wrist diameter.

Thus, $R^2$ is a measure of the fit of the model. High values of $R^2$ mean that the line predicts the data values closely, whereas lower values of $R^2$ mean that there is still a lot of variability left in the residuals (presumably due to other factors that are not measured here). What we're saying here is that there are lots of factors that account for variability in weight, and not all that can be explained by the variability in wrist diameter.


## Inference for the regression slope

The sample gives us the regression equation

$$\hat{y} = b_{0} + b_{1} x.$$

The idea of inference is that this line is meant to be an estimate of a true regression line

$$\hat{y} = \beta_{0} + \beta_{1} x.$$

In other words, if we plotted weight against wrist diameters for everybody in the world, there is, in theory, some perfect regression line that goes through the middle of all those points. And that ideal intercept and slope are the true population parameters $\beta_{0}$ and $\beta_{1}$. ($\beta$ is the Greek letter "beta".)

We have already seen that the intercept is not particularly interesting, so we restrict attention to inference for the slope. As with correlation, the full inferential rubric for the regression slope is a little overkill. Nevertheless, the rubric forces us to be careful to identify our sample and population, establish hypotheses, check conditions, and state proper conclusions.

The sampling distribution for the slope parameter is somewhat complicated. There is a formula for the standard error of the sample slope estimates, and with that, one can compute t scores that are distributed as a t model with $n - 2$ degrees of freedom. We won't get into any of the mathematical details here.

Also note that a typical regression analysis will start by checking conditions so that the regression line can be calculated and graphed. Therefore, we will work through the rubric under the assumption that the conditions have already been checked.


## Exploratory data analysis

### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

[You should type `?bdims` at the Console to read the help file.]

```{r}
bdims
```

```{r}
str(bdims)
```

We can also look at each numerical variable of interest with `favstats`:

```{r}
favstats(bdims$wgt)
```

```{r}
favstats(bdims$wri_di)
```

### Prepare the data for analysis. [Not always necessary.]

The two variables of interest, `wgt` and `wri_di`, are already coded as numerical variables.

### Make tables or plots to explore the data visually.

Here is the scatterplot with the regression line added:

```{r}
ggplot(bdims, aes(y = wgt, x = wri_di)) +
    geom_point() +
    geom_smooth(method = "lm")
```

Commentary: It would be inappropriate to show this regression line on the graph before the conditions have been checked. However, most regression analyses start by checking the conditions and writing down the equation of the regression line before doing inference on the slope parameter.


## Hypotheses

### Identify the sample (or samples) and a reasonable population (or populations) of interest.

The sample consists of 507 physically active individuals (a mix of females and males). The population is all physically active individuals.

### Express the null and alternative hypotheses as contextually meaningful full sentences.

$H_0:$ There is no relationship between weight and wrist diameter.

$H_A:$ There is a relationship between weight and wrist diameter.

### Express the null and alternative hypotheses in symbols (when possible).

$H_0: \beta_{1} = 0$

$H_A: \beta_{1} \neq 0$

Commentary: We are performing a two-sided test here. One could perform a one-sided test if the question of interest was about a positive or a negative slope specifically. Unless otherwise specified, though, the default is to run a two-sided test.


## Model

### Identify the sampling distribution model.

We use a t model with `r NROW(bdims) - 2` degrees of freedom.

### Check the relevant conditions to ensure that model assumptions are met.

The conditions have already been checked.


## Mechanics

### Compute the test statistic.

```{r}
t1 <- wgt_wrist_tidy$statistic[2]
t1
```

Commentary: `wgt_wrist_tidy$statistic` has t scores for both the intercept and the slope. Be sure to grab the 2nd entry to get the slope test statistic.

### Report the test statistic in context (when possible).

The sample slope estimate has a t score of `r t1`. The slope of the regression line predicting weight from wrist diameter is more than 26 standard errors (!) from the null slope of zero.

### Plot the null distribution.

```{r}
pdist("t", df = wgt_wrist_glance$df.residual,
      q =  c(-t1, t1),
      invisible = TRUE)
```

Commentary: The correct degrees of freedom are stored in `wgt_wrist_glance$df.residual`, not `wgt_wrist_glance$df` as you might expect.

### Calculate the P-value.

```{r}
wgt_wrist_tidy$p.value[2]
```

Commentary: Not only are there two P-values in `wgt_wrist_tidy$p.value` (one for the intercept and one for the slope), but there's one in `wgt_wrist_glance` too. The one in the `glance` output, though, is the same as the P-value for the slope, so either `wgt_wrist_tidy$p.value[2]` or `wgt_wrist_glance$p.value` will give the right answer.

### Interpret the P-value as a probability given the null.

$P < 0.001$. If there were truly no relationship between weight and wrist diameter, there would be less than a 0.1% chance of seeing data at least as extreme as we saw.


## Conclusion

### State the statistical conclusion.

We reject the null hypothesis.

### State (but do not overstate) a contextually meaningful conclusion.

There is sufficient evidence that there is a relationship between weight and wrist diameter.

### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

If we've made a Type I error, then there might not be any relationship between weight and wrist diameter, but our sample showed a relationship.


## Confidence interval

### Check the relevant conditions to ensure that model assumptions are met.

All the conditions have already been checked.

### Calculate the confidence interval.

```{r}
wgt_wrist_tidy$conf.low[2]
```

```{r}
wgt_wrist_tidy$conf.high[2]
```

Commentary: Because we used the `conf.int = TRUE` argument to `tidy` earlier, we can grab the confidence interval for the slope. But the intercept also has a confidence interval, so be sure to tack on `[2]` to grab the second entries of `wgt_wrist_tidy$conf.low` and `wgt_wrist_tidy$conf.high`, the ones that correspond to the slope parameter.

### State (but do not overstate) a contextually meaningful interpretation.

We are 95% confident that the true slope of the relationship between weight and wrist diameter is captured in the interval (`r wgt_wrist_tidy$conf.low[2]`,  `r wgt_wrist_tidy$conf.high[2]`).

### If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.

Since zero is not contained in the confidence interval, a slope of zero is not a plausible value for the relationship between weight and wrist diameter.


## Your turn

The `Rubber` data set contains data on the testing of tires. (Since it was a British study, they tested "tyres".)

Explore the relationship between the loss of tire material in an abrasion test (measured in grams per hour) and the hardness of the tire (measured in something called Shore units---Google it if you want to know more). We will consider the loss of tire material to be response and hardness to be explanatory.

Please perform the steps below, following the code examples from earlier in the module.

##### Exercise 1

Run the `lm` command and then use `tidy`, `augment`, and `glance` respectively on the output. (It's technically incorrect to run regression before checking conditions, but we need the output of `lm` in order to check those conditions.)

<div class = "answer">

```{r}
# Add code here to generate regression output with lm
```

```{r}
# Add code here to "tidy" the output from lm
```

```{r}
# Add code here to "augment" the output from lm
```

```{r}
# Add code here to "glance" at the output from lm
```

</div>

##### Exercise 2

Check conditions for regression.

<div class = "answer">

```{r}
# Add code here to check conditions for regression
```

Please write up your answer here.

</div>

##### Exercise 3

If the conditions are met, plot the regression line on top of a scatterplot of the data.

<div class = "answer">

```{r}
# Add code here to plot the regression line
```

</div>

##### Exercise 4

Write the regression equation mathematically (enclosing your answer in double dollar signs as above), using contextually meaningful variable names.

<div class = "answer">

Please write up your answer here.

</div>

##### Exercise 5

Interpret the coefficients: interpret the slope, give a literal interpretation of the intercept, and then comment on the appropriateness of that interpretation.

<div class = "answer">

Please write up your answer here.

</div>

##### Exercise 6

Report and interpret $R^2$.

<div class = "answer">

Please write up your answer here.

</div>

*****

Now we will run the full rubric for inference on the slope parameter.


##### Exploratory data analysis

###### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

<div class = "answer">

```{r}
# Add code here to understand the data.
```

</div>

###### Prepare the data for analysis. [Not always necessary.]

<div class = "answer">

```{r}
# Add code here to prepare the data for analysis.
```

</div>

###### Make tables or plots to explore the data visually.

<div class = "answer">

```{r}
# Add code here to make tables or plots.
```

</div>


##### Hypotheses

###### Identify the sample (or samples) and a reasonable population (or populations) of interest.

<div class = "answer">

Please write up your answer here.

</div>

###### Express the null and alternative hypotheses as contextually meaningful full sentences.

<div class = "answer">

$H_{0}:$ Null hypothesis goes here.

$H_{A}:$ Alternative hypothesis goes here.

</div>

###### Express the null and alternative hypotheses in symbols (when possible).

<div class = "answer">

$H_{0}: math$

$H_{A}: math$

</div>


##### Model

###### Identify the sampling distribution model.

<div class = "answer">

Please write up your answer here.

</div>

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>


##### Mechanics

###### Compute the test statistic.

<div class = "answer">

```{r}
# Add code here to compute the test statistic.
```

</div>

###### Report the test statistic in context (when possible).

<div class = "answer">

Please write up your answer here.

</div>

###### Plot the null distribution.

<div class = "answer">

```{r}
# Add code here to plot the null distribution.
```

</div>

###### Calculate the P-value.

<div class = "answer">

```{r}
# Add code here to calculate the P-value.
```

</div>

###### Interpret the P-value as a probability given the null.

<div class = "answer">

Please write up your answer here.

</div>


##### Conclusion

###### State the statistical conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### State (but do not overstate) a contextually meaningful conclusion.

<div class = "answer">

Please write up your answer here.

</div>

###### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

<div class = "answer">

Please write up your answer here.

</div>


##### Confidence interval

###### Check the relevant conditions to ensure that model assumptions are met.

<div class = "answer">

Please write up your answer here. (Some conditions may require R code as well.)

</div>

###### Calculate the confidence interval.

<div class = "answer">

```{r}
# Add code here to calculate the confidence interval.
```

</div>

###### State (but do not overstate) a contextually meaningful interpretation.

<div class = "answer">

Please write up your answer here.

</div>

###### If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.

<div class = "answer">

Please write up your answer here.

</div>
